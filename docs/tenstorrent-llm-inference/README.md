# Tenstorrent LLM Inference – Technical Deep Dive

Tenstorrent is an AI hardware startup building next-generation neural processors, with a strong focus on large language model (LLM) inference. The company’s strategy combines **innovative custom AI chips** (“Wormhole” first-generation PCIe cards) and a **fully open-source software stack** to deliver high-performance, scalable LLM inference. Tenstorrent’s hardware (based on proprietary _Tensix_ cores) has been demonstrated running state-of-the-art models like Meta’s LLaMA-70B and Alibaba’s Qwen-72B, both in single-node workstations and multi-board clusters. By 2025, Tenstorrent has open-sourced major components of its inference stack – from low-level kernel libraries to model serving frameworks – aiming to cultivate an ecosystem that can compete with Nvidia’s CUDA in flexibility and performance. This report provides a comprehensive technical deep dive into Tenstorrent’s LLM inference efforts, covering the hardware architecture, core software repositories, kernel and compiler optimizations, supported models, performance benchmarks, and future roadmap.

<div align="center">
    <img src="images/Metalium-vs-TTNN.webp"/>
    <p><em>ttnn + tt-metal</em></p>
</div>

## Inference Strategy and Goals

**Open Hardware Access and Performance:** Tenstorrent’s inference strategy centers on exposing the full capabilities of its AI chips to developers. To that end, the company provides a **bare-metal programming model (“Metalium”)** that allows expert users to write custom kernels and orchestrate data movement at the lowest level. This is akin to a “CUDA for Tenstorrent,” enabling fine-grained performance tuning by directly controlling the _Tensix_ cores and on-chip memory. While only a small subset of users will write custom kernels, Tenstorrent views this openness as vital for squeezing out efficiency in production deployments. In parallel, Tenstorrent offers higher-level APIs and automation (via its TT-NN library and compiler stack) so that most users can run models out-of-the-box without dealing with hardware details.

**Scalable Multi-Chip Inference:** A key goal is to run **very large models and many concurrent queries** by scaling across multiple chips. Tenstorrent’s first-gen Wormhole cards were designed for scale-out: each chip includes **16× 100 Gbps interconnect lanes** that allow building multi-card meshes (the 32-chip _Galaxy_ system) without external switches. The inference software supports _tensor parallelism_ to shard giant models across devices and _data parallelism_ to serve many requests in parallel. For example, LLaMA-70B has been run with **tensor-parallel 8-way splitting on 8 cards**, and Falcon-40B was demonstrated on a **32-chip Galaxy** server. This scale-out approach lets Tenstorrent treat a cluster of chips as one large memory and compute resource for LLMs – the Galaxy appliance is essentially pitched as an **LLM inference server** in a box.

<div align="center">
    <img src="images/Galaxy-system.webp"/>
    <p><em>Tenstorrent’s Galaxy system comprises 32 Wormhole chips connected in a mesh</em></p>
</div>

**High-Throughput, Acceptable Latency:** Tenstorrent optimizes for high throughput (tokens/sec) under multi-user server loads while keeping latency per user reasonable. Internally, they use the metric _tokens per second per user_ (t/s/u), i.e. how fast each user receives tokens given a certain batch of concurrent users. The company considers **≥10 tokens/sec/user** sufficient for responsive chat applications. Their **Loud Box** workstation (8 Wormhole cards) currently achieves \~15 tokens/s per user with 32 users on LLaMA-70B, which is already in the interactive range. This performance is approaching the ballpark of Nvidia H100 systems (cloud APIs on 8×H100 report \~20–50 tokens/s/user). Tenstorrent’s near-term goal is to **double single-user speed via software optimizations** (e.g. improved kernels, speculative decoding) on the same hardware, and to further close the gap with second-gen chips. At the system level, Tenstorrent positions itself as a cost-efficient alternative – their 8-card workstation (\~\$12k) targets developers who need on-premise LLM serving, compared to the expensive DGX boxes used by hyperscalers.

**Framework Integration and Ease of Use:** Acknowledging that most AI engineers work with established frameworks, Tenstorrent’s stack is designed to integrate with front-ends like PyTorch, TensorFlow, ONNX, and even LLM-specific serving frameworks. The strategy involves a **top-down compiler** (to import models from various frameworks and optimize them for Tenstorrent chips) and a **runtime that can plug into inference servers** (so that using a Tenstorrent back-end is as seamless as swapping a device type). For instance, Tenstorrent built a custom fork of the popular high-throughput serving engine **vLLM** to interface with their hardware, enabling features like continuous batching and efficient memory management for LLM workloads on Tenstorrent devices. They also provide an [**Inference Server repository**](https://github.com/tenstorrent/tt-inference-server) with ready-made model API implementations and Docker images, aiming to let users deploy common models on Tenstorrent cards with minimal setup. Overall, Tenstorrent’s inference strategy is to combine **open, low-level control for maximum performance** with **easy adoption paths via high-level tools**, thereby appealing both to performance-tuning experts and to AI teams wanting a plug-and-play solution.

## Key Repositories and Software Stack

Tenstorrent’s LLM inference software stack spans several actively maintained open-source repositories, each addressing different layers of the stack:

<div align="center">
    <img src="images/tenstorrent_software_stack.webp"/>
    <p><em>Tenstorrent’s software stack</em></p>
</div>

- **TT-Metal (TT-NN and TT-Metalium)** – _Operator Library and Kernel Runtime:_ This is the core Python/C++ library for executing neural network operations on Tenstorrent hardware. **TT-NN** provides a tensor class (`ttnn.Tensor`) and dozens of operators (akin to PyTorch’s ATen or CUDA’s cuDNN) implemented for the Tenstorrent architecture. It allows creating and running computation graphs on the device using familiar tensor semantics. Under the hood, TT-NN builds on **TT-Metalium**, Tenstorrent’s low-level programming model for the hardware. TT-Metalium exposes the _Tensix_ core architecture in a C++ API, letting developers write custom device kernels by orchestrating the five RISC-V sub-cores, DMA engines, and matrix units (more details in the next section). In practice, most developers use the high-level TT-NN ops, while a few expert users or Tenstorrent engineers implement new optimized kernels in Metalium when needed. The TT-Metal repository includes extensive documentation and even _tech reports_ on topics like the Matrix Engine and data formats. It also contains _model demo scripts_ for various networks and a performance table (updated as of May 5, 2025) showcasing throughput and latency across models and hardware configurations. TT-Metal is under active development with thousands of commits and a robust issues/pull-requests workflow, reflecting its central role in Tenstorrent’s software. (Notably, Tenstorrent uses the term “TT-Metal” sometimes to refer broadly to this whole stack, but technically TT-Metalium is the metal-level API and TT-NN is the neural network library.)

- **TT-Forge (MLIR-Based Compiler)** – _Graph Compiler and Runtime Planner:_ TT-Forge is Tenstorrent’s newer compiler framework (successor to an earlier stack called _Buda_). It is built on LLVM’s MLIR and serves as a **high-level compiler that ingests models from popular frameworks and lowers them to Tenstorrent executable code**. The idea is to enable a model trained in PyTorch or TensorFlow to be deployed on Tenstorrent silicon with automated graph optimization, placement, and scheduling. TT-Forge’s front-end (tt-forge-fe) can take in PyTorch JIT IR, ONNX graphs, etc., and perform transformations like operator fusion, layout conversion, and graph rewriting for performance. The compiler then leverages **TT-MLIR** passes to target the Tensix core architecture. The output is a low-level “netlist” or program that maps each neural net operation to specific Tensix cores and schedules data movement – a human-readable form of this mapping is available for inspection before it’s assembled into a binary for execution. TT-Forge is intended to be general and open, supporting all configurations of Tenstorrent hardware via a single, performance-oriented compiler. In essence, TT-Forge and its components automate what a skilled developer might otherwise do by hand with TT-Metalium: partitioning the model across cores/devices, choosing tiling strategies, and inserting data transfers. It’s actively updated (as of May 2025) and is a key piece for scaling to new models and future chips.

- **PyTorch Integration (torch.compile backend)** – Tenstorrent maintains an integration with PyTorch 2.x through a project often referred to as **pytorch2.0_ttnn**. This provides a backend for PyTorch’s `torch.compile()` (AOT compilation) so that users can take a PyTorch model and just compile it for Tenstorrent, similar to how one would for XLA or other accelerators. It uses the TT-NN compiler under the hood (TT-Forge/MLIR) to translate PyTorch ops into TT-Metal ops and kernels. By supporting PyTorch’s compiler API, Tenstorrent enables researchers to test models on their hardware with minimal code changes – essentially treat the Tenstorrent card like another device in the PyTorch ecosystem. Alongside this, Tenstorrent also has a **PJRT device for XLA/JAX** (the `tt-xla` repo) and a **TVM integration** (`tt-tvm` repo), underscoring their approach of meeting developers where they are. These compiler integrations are crucial for leveraging Tenstorrent hardware in existing ML workflows for both training and inference, although in practice by 2025 the emphasis is on inference deployment.

- **TT-Inference-Server** – _Model Serving APIs and vLLM Integration:_ This repository provides ready-to-use server implementations for popular LLMs on Tenstorrent hardware. It essentially packages **a modified vLLM** (open-source LLM serving engine) with Tenstorrent-specific backend hooks, along with model-specific configurations and API endpoints. The included **“Model Readiness” table** lists each supported model, a link to its Hugging Face weights, the recommended Tenstorrent hardware for it, and the software version (TT-Metal and vLLM commit) it’s compatible with. For example, entries include _Llama-3.3-70B-Instruct_ on a LoudBox (8 cards) marked “ready,” or _Qwen2.5-7B_ on a single n150 card, etc.. Each model name in the table links to an implementation – typically a small Python module defining how to load the model weights into TT-NN, any model-specific pre/post-processing, and integration with vLLM’s engine. The server leverages **vLLM’s high-throughput scheduler** (which uses a “paged attention” mechanism to efficiently handle concurrent requests) but replaces GPU-specific operations with calls into TT-NN (TT-Metal ops). In effect, TT-Inference-Server + vLLM gives users a **HTTP/GRPC serving interface** for LLM inference on Tenstorrent, with features like dynamic batching, streaming token output, etc., similar to a HuggingFace TextGenerationInference server but optimized for Tenstorrent. The repository also provides **Docker containers** for quick deployment of these inference servers. This component underscores Tenstorrent’s focus on _actively maintained, ready-to-run inference solutions_ – one can spin up a container and serve a model on a Tenstorrent box without having to write any kernel code or graph optimization by hand.

- **Support Utilities:** In addition to the main stack above, Tenstorrent provides tools like **TT-SMI** (a CLI tool similar to `nvidia-smi` for monitoring cards), **TT-Topology** for multi-card ethernet topology configuration, firmware flashing tools (TT-Flash), and visualization/debug tools (e.g. **ttnn-visualizer** for graph and memory visualization). These support inference deployment by giving insight into performance and helping manage hardware. For debugging or development, a **TTNN “graph trace” and profiling** feature exists to inspect how ops are executed on Tensix cores. Tenstorrent also maintains example **model zoo repos** (like `tt-buda-demos` and in TT-Metal’s docs) that show how to run various models (CNNs, Transformers, Whisper, Stable Diffusion, etc.) with their stack – many of these double as integration tests and performance benchmarks for inference.

In summary, Tenstorrent’s inference stack ranges from _low-level (Metalium ISA, custom kernels)_ to _high-level (model servers and PyTorch integration)_, with each layer open-sourced. The actively maintained repos ensure that new models and techniques can be rapidly incorporated. For instance, when Meta released Llama 3.2/3.3 models in late 2024, Tenstorrent’s team updated the model table and added support for Vision and Instruct variants, illustrating the agility of an open-source approach. Table-driven model support, combined with a robust compiler and core library, makes it possible to extend support to other architectures (they even issued bounties for community contributions to add models like _Gemini_ and _Phi-2_ to TT-Buda/Forge). This flexible software foundation is crucial for handling the fast-moving LLM landscape.

## Architectural and Kernel Optimizations

_Hardware Overview:_ Tenstorrent’s LLM inference optimizations are deeply tied to the design of its **Tensix cores** and memory architecture. A Tensix core is a **spatially-programmable compute unit** with independent control, rather than a SIMD lane or fixed-function MAC array. Inside each core are multiple RISC-V micro-cores and specialized engines: _two RISC-V “router” cores_ handle data movement (DMA and NoC transfers), and _three RISC-V “compute” cores_ issue vector and matrix-multiply instructions to the core’s math units. The math units consist of an FPU (which in Tenstorrent terminology is actually a **matrix multiply engine**) and an SFPU (a **vector/SIMD engine** for elementwise ops). Each core also includes a chunk of **SRAM (L1 memory)**, which is tightly coupled. The first-gen chips (_Grayskull_ and _Wormhole_) have on the order of 80–120 Tensix cores and \~100+ MB of total on-chip SRAM. These cores are connected by a high-bandwidth, circuit-switched network-on-chip. Importantly, in the [Wormhole](https://tenstorrent.com/en/hardware/wormhole) design, multiple chips can be directly linked via 100 Gbps Ethernet-style links and **coherent memory networking** – a core on one chip can fetch data from another chip’s SRAM almost as if it were local. This architecture is optimized for **fine-grained parallelism and pipeline parallelism**. Unlike a GPU where hundreds of threads execute in lock-step, Tensix cores run **asynchronously (MIMD)** – each core can be working on a different part of the computation or even on a different layer of the model, communicating via on-chip routers when needed. This affords a lot of flexibility in mapping an LLM’s computation graph onto the hardware.

<div align="center">
    <img src="images/Tensix-core.webp"/>
    <p><em>Inside a Tenstorrent Tensix core, BRISC and NRISC handle data movement into and out of the core, while TRISC0, TRISC1, and TRISC2 manage the math engines. The FPU serves as a matrix-multiply accelerator (despite its name suggesting a floating-point unit), and the SFPU functions as a vector engine.</em></p>
</div>

_Kernel (Metalium) Execution Model:_ To keep the many MAC units busy, Tenstorrent relies on a **decoupled execution model** – separating computation from data movement. As VP of HW Matt Mattina explained, _“math is separate from unpacking and packing data”_, and both run in parallel. In practice, this means a TT-Metalium kernel might dedicate some Tensix cores solely to staging data (bringing in next tokens’ data from DRAM or distributing key/value cache across chips) while other cores multiply matrices, and yet others move results around. The TT-Metalium API allows explicit scheduling of work to RISC-V threads on each core, rather than relying on implicit caching or global barriers. For example, a custom attention kernel could use one thread to load query and key tiles into SRAM, another to compute partial dot products (matrix multiplies), and a third to stream out the computed attention scores – all overlapping in time. This is reminiscent of manually orchestrating pipeline parallelism at the kernel level, which Tenstorrent enables through Metalium. Such **explicit control of dataflow** is crucial given the distributed memory: it ensures that the theoretical peak of \~2,000 multiply-accumulates per cycle per core can be approached by keeping data moving continuously. In contrast to CUDA where warps are implicitly synchronized each instruction, Metalium lets each core run its own instruction stream, only synchronizing when and where the programmer wants. This freedom allows implementing _models with different parallelization strategies_ – e.g. one could assign a subset of cores to each transformer layer (pipeline the layers through core groups), or do each matrix multiply across many cores in parallel, or combinations thereof. Tenstorrent’s compiler (TT-Forge) typically handles this mapping automatically, but advanced users can tweak the placement via the netlist if needed.

**Data Layouts and Memory Optimization:** One notable optimization in Tenstorrent’s inference stack is the use of **custom tensor layouts** to match hardware. Tenstorrent does _not_ use standard row-major order in memory for multi-dimensional tensors, because their hardware benefits from tiling data to fit SRAM and align with matrix engine dimensions. For instance, they use a tiled layout for matrices so that blocks of the matrix reside in contiguous SRAM segments (enabling better locality when cores multiply sub-blocks). This non-row-major format improves on-chip reuse but required significant work in the software: the TT-NN library must transform (“format”) incoming weights/activations into the tiled layout expected by kernels. The TT-NN API includes utilities like `ttnn.format_input_tensor()` and `pad_to_tile_shape()` to help prepare data. A developer integrating Tenstorrent with a new framework observed that _“the use of non row-major layout is the main troublemaker… Everyone assumes each other speaks row-major, so an adapting interface has to be made”_. This highlights a challenge: while these layouts boost performance, they necessitate careful data conversion at boundaries (e.g. when copying weights from CPU memory into the accelerator’s memory). Tenstorrent’s solution is to handle this transparently in their compiler/loader – when a model is imported via PyBuda or TT-Forge, it automatically converts the weight matrices into the optimal internal format. At runtime, **all operations on Tenstorrent devices expect and produce tensors in these native layouts**. This can complicate mixing Tenstorrent with other accelerators in a pipeline (due to conversion overhead), but it ensures that _within_ the Tenstorrent execution, memory access is efficient. The benefit is evident in performance scaling: even at very high batch sizes or with many cores active, the chip is not starved for data because the data movement threads and tiling keep caches warm and memory accesses coalesced.

**Precision and Numeric Optimizations:** In LLM inference, using lower numerical precision can significantly speed up throughput and reduce memory usage. Tenstorrent heavily leverages **8-bit floating point** formats for inference. The Wormhole chip supports FP16/BF16 as well, but its peak theoretical is 74 TFLOPS (FP16) versus 262 TFLOPS in _FP8_ on an n150 card. In practice, Tenstorrent runs large LLMs at **Block FP8 (BFLOAT8)** precision for generation. Their BF8 format is a kind of block-floating scheme (shared exponent) that balances range and precision. Using BF8, a 70B model’s weights are roughly half the size of BF16, enabling a model like Llama-70B to fit on 8 cards (each card with 12 GB GDDR6) with room for activation memory. The TT-Metal software handles weight quantization to BF8 and any required dequantization. They note in documentation a _limitation of BFLOAT8_B_ (likely minor accuracy considerations), but overall have found it effective for LLMs. Empirically, running Llama-70B in BF8 on Tenstorrent produced coherent outputs in their demos (the model quality remained high). Beyond FP8, Tenstorrent cores also support _int8_ and other low-precision dot products – though primarily for vision models or experimentation. The software allows mixing precision: for example, one could keep embedding layers in BF16 and transformer blocks in BF8 if needed for accuracy. However, **nearly all their published LLM benchmark numbers use 8-bit** arithmetic for maximum throughput. This is a conscious optimization to compete with Nvidia’s TensorRT and FasterTransformer libraries, which also use 8-bit or 4-bit quantization for serving large models. Tenstorrent’s advantage is that BF8 is natively supported in hardware (no extra latency for dequantization on tensor cores, etc.), so the throughput scales almost linearly with the reduced precision. Kernel-wise, Tenstorrent likely implemented custom **matrix multiplication kernels** for BF8 in Metalium to accumulate into higher precision (similar to how Nvidia does FP8 => accumulate in FP16). The performance table confirms massive gains: e.g. Llama3.1-70B at BF8 on 8 chips hits \~15 tokens/s/user (as noted), which would be much lower if FP16 were used (they set a “target” of 20 tokens/s/user for 70B, presumably at BF8, whereas FP16 might only reach half that).

**Multi-Device Parallelism:** On the software side, Tenstorrent’s inference stack supports **Tensor Parallel (TP)** and **Data Parallel (DP)** distribution seamlessly. In their [model performance table](https://github.com/tenstorrent/tt-metal?tab=readme-ov-file#llms), entries like “(TP=8)” indicate a model’s layers were split across 8 devices, each device holding 1/8 of the model weights. The TT-Forge compiler can partition the weight matrices accordingly and insert the necessary all-reduce or scatter/gather operations over the NoC/Ethernet links. Thanks to Wormhole’s built-in 100G ×16 links, the communication overhead for TP is relatively low – for instance, Qwen-72B with TP=8 on a QuietBox (8 cards) achieved \~14.5 tokens/s/user, which is in line with the Llama-70B numbers and suggests the scaling is efficient. For Data Parallel, Tenstorrent can broadcast a copy of the model to multiple devices and have each handle different batch slices (or different users). The table shows Falcon-7B scaled to DP=32 (across 32 chips) serving 1024 concurrent prompts at \~4.8 tokens/s each – an aggregate of nearly 5k tokens/s throughput. This demonstrates nearly linear scaling of throughput with more devices in data-parallel mode (32× increase in batch yielded \~32× total tokens/sec). Behind the scenes, such scaling is facilitated by _Tenstorrent’s graph compiler and runtime which manage synchronization and networking._ The **Wormhole chips treat remote memory access almost like another level of cache**, as noted, making it simpler to program model parallelism; e.g. a core can fetch weight chunks from a neighboring chip’s SRAM during a TP layer computation. The TT-Metal runtime abstracts this so that developers or the compiler can largely write parallel code without micromanaging low-level DMA for off-chip transfers – the hardware’s NIC and Metalium’s API can handle inter-chip copy operations (likely via special instructions or channels). This tight integration of compute and networking is a big architectural optimization for inference at scale, where multi-GPU setups often suffer from latency or limited interconnect bandwidth.

**Attention and Caching:** LLM inference is bottlenecked not only by matrix multiplies but also by the _attention mechanism_, which involves memory-intensive operations (e.g. large key/value caches). Tenstorrent’s approach to attention combines hardware memory advantages with software scheduling. Each Tensix core’s SRAM can act as a **scratchpad for storing a segment of the KV cache**. In a 80-core chip, the \~96 MB SRAM could hold a few thousand tokens of a 70B model’s keys and values (distributed across cores). For longer contexts (e.g. Qwen-72B supports 128K tokens), Tenstorrent likely streams the cache from off-chip GDDR6 as needed. The vLLM-based server uses a **“paged KV cache”** concept, which fits well with Tenstorrent’s architecture – it breaks the cache into blocks that can be loaded into SRAM on-demand. Since Tensix cores can individually fetch data, one can imagine cores cooperating such that when attention for token N is computed, cores pull only the relevant key blocks for the current query chunk, rather than keeping the entire 128K history on-chip. This resembles how vLLM handles GPU memory, but here the advantage is each core can directly access other chips’ SRAM over the NoC, effectively giving a larger pool of “unified” memory for KV. Additionally, Tenstorrent may have implemented **FlashAttention-like kernels** in Metalium (FlashAttention algorithm computes attention in tiled fashion to avoid storing full intermediate matrices). Given Metalium’s flexibility, a custom kernel could perform the Q·K^T multiply and softmax in chunks that fit in SRAM, eliminating large intermediate allocations. While specifics aren’t published, the high throughput for attention-heavy models suggests such optimizations. For instance, the _DeepSeek-R1 70B Distilled_ model (which likely has long-chain-of-thought prompts) still achieves \~15.2 tokens/s/user on 8 chips, indicating the attention step isn’t a major bottleneck there. Tenstorrent’s documentation notes that their reported t/s/u is measured **after the prompt prefill** (i.e., generation phase tokens/sec), which means the numbers account for attention + decode, and they are consistently high. This implies careful optimization of the attention cache accesses and compute. Another kernel-level trick is **speculative decoding** (generating multiple tokens ahead and verifying them), which Tenstorrent hasn’t implemented yet but is on their roadmap to further boost throughput and latency.

In summary, Tenstorrent’s architectural and kernel optimizations for LLM inference include: **(1)** decoupled execution with parallel data movement to keep compute busy, **(2)** custom tensor layouts and tiling to maximize on-chip SRAM re-use, **(3)** aggressive use of FP8 precision to increase effective compute and memory bandwidth, **(4)** built-in model/data parallel scaling with fast interconnects and a compiler that partitions models across cores/devices, and **(5)** specialized handling of attention and long context via paged caches and possibly fused kernels. These efforts collectively allow Tenstorrent hardware to approach state-of-the-art inference speeds. A notable point is that **Tenstorrent’s stack is still evolving** – the company acknowledges current software isn’t fully utilizing the hardware yet (they see room to double performance via software). The open Metalium model is a double-edged sword: it provides unmatched control, but writing optimal kernels is complex (the programming model is unlike CUDA and took developers time to learn). Tenstorrent is mitigating this by improving the compiler and providing more pre-optimized kernels for common patterns (they note “3 or 4 design patterns will cover 90% of use cases” in custom kernels). Over time, as the software matures, we can expect further tuning – e.g. more of the manual best practices (overlap compute/communication, etc.) will be automated by TT-Forge, and more ops (like various activation functions or layernorms) will get bespoke implementations to avoid any leftover inefficiencies.

## Supported Models and Use Cases

Tenstorrent’s inference efforts currently support a wide array of **LLMs and related AI models**, with an emphasis on the most popular open LLMs used for chatbots, coding assistants, and research. The actively maintained [model list](https://github.com/tenstorrent/tt-inference-server?tab=readme-ov-file#llms) (as of 2025) includes:

- **Meta LLaMA Family:** Tenstorrent supports _Llama 3.1, 3.2, and 3.3_ series models from 1B up to 70B parameters. This covers base models as well as instruction-tuned variants and a multimodal vision model. For example, **Llama-3.3-70B-Instruct** (Meta’s 70B chat-tuned model from late 2024) is marked “✅ ready” on Tenstorrent’s server, validated on an 8-card Quiet/Loud Box system. They also run the **Llama-3.2-11B-Vision** model (an image+text model requiring visual input), demonstrating support for multimodal inference. Smaller Llamas like 1B, 3B, 8B are fully supported on single cards (useful for edge cases or development). Essentially, Tenstorrent has kept pace with Meta’s major LLM releases – Llama2 from mid-2023 (which corresponds to Llama 3.1 naming in their table) through Llama 3.3 at end of 2024 – ensuring that customers can deploy these state-of-the-art models.

- **Alibaba Qwen Series:** Qwen is a family of Chinese-English bilingual LLMs introduced by Alibaba. Tenstorrent supports **Qwen-2.5** models, including the 7B and 72B parameter versions, and both base and instruct fine-tunes. Qwen-2.5 (released late 2024) is notable for its 128K context length and strong math/code abilities. Tenstorrent was quick to integrate Qwen; their performance table lists Qwen-7B (TP=2 on one n300 card) and Qwen-72B (TP=8 on 8 cards), both achieving around 14–32 tokens/sec per user. This indicates that even the long-context features of Qwen are supported – likely by using external memory for KV and bringing pages into SRAM as needed. By supporting Qwen, Tenstorrent targets use cases in Asia and any applications needing multi-lingual capabilities or extreme context lengths (like long document Q\&A).

- **DeepSeek-R1 Models:** DeepSeek is a research initiative that produced very large “reasoning” models and smaller distilled variants (with an emphasis on mathematical and logical reasoning). Tenstorrent collaborated to support **DeepSeek-R1-Distill-Llama-70B**, a 70B model distilled from DeepSeek’s RL-trained LLM. This model is open-source and represents cutting-edge academic work in early 2025. Tenstorrent’s inclusion of it (in _preview_ status) shows a willingness to work with community models beyond the mainstream. The DeepSeek distilled model likely pushes long-chain reasoning workloads; Tenstorrent managed \~15 t/s/user on it with 8 cards, demonstrating that even non-standard model variants run efficiently. They have also indicated support for DeepSeek-distilled Qwen-32B (“QwQ-32B”), which appears in the table – this might be DeepSeek’s 32B Qwen-based model (nicknamed “QwQ”). Running such models serves use cases in advanced reasoning tasks, potentially scientific QA or complex multi-step reasoning assistants.

- **Falcon and Mistral:** These are other open-source LLMs that Tenstorrent supports. **Falcon-40B** (from Technology Innovation Institute) and **Falcon-7B** are supported, including multi-chip scaling. Notably, Falcon-40B (40B parameters) was run with TP=8 on 8 chips achieving \~36 tokens/sec total throughput, and Falcon-7B was scaled to large DP batches (as mentioned, 1024 contexts on 32 chips). **Mistral-7B**, a high-quality 7B model released in late 2023, is also supported and listed in the table. Additionally, Tenstorrent experimented with “Mixtral 8×7B”, which appears to be an ensemble or MoE of 8 Mistral models (TP=8) on 8 chips. Mixtral yields \~16.6 tokens/s/user at batch 32, indicating an interesting use case: ensembles of smaller experts instead of one big model. This could be useful for ensemble-based inference (e.g., to boost accuracy or diversity of responses) if the application can tolerate ensemble voting.

- **Other Domains (Vision, Speech, etc.):** While the focus is LLMs, Tenstorrent’s inference stack also supports non-LLM models, some of which are often used alongside language models. For example, **Whisper (distil-large-v3)**, a speech-to-text model, is supported and runs with good speed (54.7 sentences/sec on a single n150 card). On vision, classic models like **ResNet-50** and **YOLOv4** have highly optimized kernels on Tenstorrent – ResNet-50 hits up to 96,800 images/sec on 32 chips, showing near-linear scaling. They also run Stable Diffusion (a generative vision model) as a demo of GPU-equivalent capability in generative AI beyond text. However, these vision/CV models are mostly proof-points; Tenstorrent’s marketing and engineering in 2024–2025 clearly prioritize **LLM inference as the flagship use case** (given the attention to multi-user LLM performance and partnerships around those models).

**Primary Use Cases:** The supported model list indicates Tenstorrent is targeting use cases like: **Chatbots/Q\&A assistants** (Llama, Falcon, Qwen instruct models) – where multi-user throughput and reasonable latency are crucial; **Code assistants** (Llama and Qwen are strong at coding tasks) – likely of interest to developer tool companies; **Multilingual assistants** (Qwen excels in Chinese and others, Llama supports many languages) – appealing to global or non-English markets; **Long-context document analysis** (Qwen-2.5’s 128K context means you could feed entire books or logs for analysis); and specialized tasks like **complex reasoning or math problem solving** (DeepSeek models, which aim at OpenAI-level reasoning benchmarks). The presence of Whisper suggests use in transcription or AI assistants that handle voice. Vision+LLM (Llama-3.2 Vision) implies future applications for multimodal assistants (describe image + answer question, etc.), which Tenstorrent’s hardware can handle due to flexible kernel support for both CNNs and transformers in one device.

From an internal standpoint, Tenstorrent likely has customers or pilot users in fields like finance (who want their own GPT-style model in-house), academia (who need hardware to experiment with large open models), and defense/government (which prefer on-prem hardware, not cloud GPUs, and open software for security reasons). Jim Keller (Tenstorrent’s CEO) mentioned their typical customer is one who wants to **buy hardware and run open models themselves** rather than use cloud APIs. Therefore, supporting all major open LLMs is a strategic must – and the rapid updates to their repository (with Llama 3.x and Qwen2.5 integrated soon after release) show they’re meeting that demand.

**Validation and Status:** It’s worth noting Tenstorrent labels models as “✅ ready” or “\[preview]” in their documentation. “Ready” means the model has been validated to run stably on hardware and likely meets performance expectations. Many smaller models (1B–7B) and established ones (Llama 70B, etc.) are ready, whereas very new or experimental ones are marked preview. For example, in the snippet, DeepSeek-R1 Distill 70B and Qwen-72B were in preview as of the latest update, indicating ongoing work to optimize and harden them. This transparent status tracking is important for internal planning – it highlights **blockers** to be solved (e.g., perhaps the preview models had stability issues or needed further kernel tuning, which would be actively addressed by the team). We will discuss these challenges next.

## Recent Performance Benchmarks and Results

Tenstorrent has made impressive progress in LLM inference performance, as evidenced by both their internal benchmarks and third-party demos. Here we summarize recent results (2024–2025) and compare to industry standards:

- **Llama-70B on Loud Box (8× Wormhole cards):** Achieved **15 tokens/sec per user with 32 concurrent users** (after prompt, in generation mode). This corresponds to a throughput of \~480 tokens/sec total, with latency per token around 66 ms. For human perception, this means the system can comfortably generate a few sentences per second for each of 32 users simultaneously – effectively real-time chatbot responses. _EE Times_ reported this demo and noted that **10+ tokens/sec/user** is generally considered smooth for chat, so Tenstorrent exceeding 15 tokens/sec is a strong result. By comparison, Nvidia’s 8×H100 can push further in pure throughput (they mention 20–50 t/s/user possible in hyperscaler setups), but the cost/performance ratio of Tenstorrent is a highlight (achieving \~15 with a \~\$12k machine vs \~30 with a \$300k DGX). More importantly, Tenstorrent believes with software refinements they can double this to \~30 t/s on the same hardware – which would squarely hit the lower end of H100 performance range but at a fraction of cost, a very compelling proposition if realized.

- **Galaxy 32-chip system results:** In a fully scaled 32-chip _Galaxy_ rack (which uses either 32 n150 cards or 16 n300 dual-chip cards), Tenstorrent demonstrated running **Falcon-40B** and other models. Falcon-40B on Galaxy (with data parallel) was shown to serve _1024 concurrent_ prompts at \~4.8 tokens/s each. That is an aggregate of \~4,915 tokens/sec (first token latency \~223 ms for batch 1024). This kind of capability positions Galaxy as an “LLM appliance” for massive multi-user deployments – for instance, an enterprise could host an internal 40B-parameter assistant for thousands of employees on one 32-chip Galaxy. Another striking number: **Llama-70B on 32 chips (TP=32)** reached _80 tokens/sec per user_ (with batch 32). This was with a variant of Llama-3.1-70B, and a _time-to-first-token of only 45 ms_ was recorded. That suggests near-interactive latency even on the first token for a large model, due to parallelizing the model across so many cores/devices. 80 tokens/sec per user is extremely high – far above typical real-time needs – indicating that in batch scenarios the system can blast through text generation tasks (this could be useful for processing large texts or logs in parallel, or simply showing the raw capability in less constrained settings). These numbers also reflect the **efficiency of Tenstorrent’s scale-out**: the combination of high intra-chip bandwidth and the compiler’s parallelization means adding more chips largely multiplies performance.

- **Smaller models on single cards:** On a single Wormhole card (n300d with 2 chips, or n150 with 1 chip), Tenstorrent’s performance for 7B–13B models is excellent. For example, **Qwen-7B** on one n300 (TP=2 across its two chips) hits **32.5 tokens/sec/user at batch=32**. That’s \~1,040 tokens/sec total throughput from one card, which is on par with an Nvidia A100 GPU running similar model (for context, an A100-40GB could do \~1200 tokens/sec on 6B GPT-J in some tests; Qwen-7B is similar scale, so Tenstorrent is in the same ballpark). Another data point: **Mistral-7B** on one n150 card produces \~25.8 tokens/sec/user (batch 32), which equates to \~826 tokens/sec total. These indicate that even without scaling out, a single Tenstorrent card can serve a decent number of users for smaller models. Latency-wise, time to first token for Mistral-7B was 240 ms (batch 32) – likely due to not splitting the model (7B fits in one chip comfortably, so they didn’t use TP). 240 ms TTF for 7B is still okay for a first response. For larger batches, they also show how data parallel mode increases throughput: Falcon-7B on 8 chips (DP=8, batch 256) gave \~15.5 tokens/sec/user, versus \~18.3 t/s on single card batch 32 – indicating some network overhead at very high batch but generally scaling well.

- **Precision/Quality:** All these performance numbers leverage BF8 precision. Internally, Tenstorrent likely validated that BF8 inference preserves model accuracy sufficiently. They haven’t published detailed accuracy comparisons, but given industry experience with LLM quantization, a well-chosen FP8 format can keep loss <1 perplexity point difference from FP16. The DeepSeek team’s model results (which Tenstorrent runs in BF8) achieved new state-of-the-art dense model scores, implying that Tenstorrent’s BF8 didn’t hinder those results. Additionally, Tenstorrent can fall back to higher precision if needed for certain parts (and their software allows mixing). As a result, none of the external reports noted quality issues – the focus was purely on throughput and cost.

- **Comparisons and Rankings:** As of mid-2025, Tenstorrent’s LLM inference performance is competitive among AI startups. EE Times noted the wave of single-user LLM performance announcements by Groq, SambaNova, Cerebras (all offering Llama-based API services). Tenstorrent’s approach is slightly different (selling hardware rather than cloud API), but if we compare raw speeds: Groq claimed >100 tokens/sec on Llama-70B (single-stream) by using massive model parallelism on their chips; Cerebras can hit \~30 tokens/sec on 70B with their Wafer-Scale Engine (single user). Tenstorrent with 8 cards is targeting \~30 tokens/sec (with improvements), which is in that mix. For many-user throughput, Tenstorrent’s numbers (e.g. 32 users × 15 t/s or 1024 users × \~5 t/s) are very strong, thanks to their vLLM integration allowing high batch utilization. It’s clear Tenstorrent excels in **throughput per dollar** if not yet outright speed per stream. They’ve convincingly shown that with an open-stack, moderate-power solution, one can achieve near-H100 performance on inference tasks.

- **Qualitative benchmarks:** It’s not just about tokens/sec – Tenstorrent also demonstrated the _end-to-end viability_ of their stack by generating entire responses on real prompts. For example, [Martin (developer blog) showed a TinyLlama 1.1B generating tokens correctly via TT-Metal integration](https://clehaxze.tw/gemlog/2024/10-28-generating-the-1st-token-on-tenstorrent-with-ggml.gmi), and Tenstorrent’s own Developer Day likely showcased live demos (though we lack the transcripts). The stability of the software has improved to run long sessions: one dev update noted they got TinyLlama to generate until end-of-text without crashing after some fixes – a milestone indicating the runtime can handle full sequences and proper memory management.

To sum up, **recent benchmarks underscore that Tenstorrent’s first-gen hardware, combined with their evolving software, is capable of serving large language models at scale with performance in the same order of magnitude as the leading GPU solutions**. The numbers are already sufficient for many use cases (e.g., internal chatbots, batch processing, edge deployment of 7B–13B models), and there is a clear trajectory for improvement.

One caveat: all these results were obtained with **actively maintained code and close hardware-software co-design**. In practice, Tenstorrent’s engineers have often been directly involved in tuning these model demos (as evidenced by the rapid addressing of issues on their trackers and their hands-on demos to press). The user experience will continue to improve as these optimizations are rolled into the public releases (e.g., the aim to double Llama-70B speed via software updates means users can expect better performance with software version upgrades without changing hardware). It will be critical to follow the **release notes and model support table** – Tenstorrent regularly updates those with any new performance figures or model capabilities, ensuring transparency on where things stand.

## Future Plans and Roadmap

Tenstorrent’s roadmap for LLM inference spans both **next-generation hardware** and **major software advancements**:

- **Second-Generation Chips (Blackhole & Beyond):** Jim Keller has stated that the upcoming **“Blackhole” chips (2nd-gen)** will _“double or triple performance”_ relative to Wormhole. From Hot Chips disclosures, Blackhole is a [**standalone AI SoC**](https://hc2024.hotchips.org/assets/program/conference/day1/88_HC2024.Tenstorrent.Jasmina.Davor.v7.pdf) with 120 improved Tensix cores on 12nm, on-package memory (100 GB/s LPDDR4), and integrated 400 Gbps Ethernet interfaces. In essence, Blackhole can be thought of as a beefed-up Wormhole with more cores and its own CPU and memory (no host needed). For inference, this means a single Blackhole board could likely replace an 8-card Wormhole system in performance, simplifying deployments. The integration of RISC-V CPU cores to run Linux on the chip (hinted by “standalone”) also opens the door to **embedding the model server directly on the accelerator**, reducing latency and system complexity. Tenstorrent expects Blackhole to significantly improve single-stream latency (with higher clock and core count) and multi-stream efficiency (with its networking enabling larger mesh without external NICs). Blackhole is projected for availability perhaps in late 2025, and Keller is confident in hitting much better numbers on LLM benchmarks with it.

<div align="center">
    <img src="images/AI-Silicon-Roadmap.png"/>
    <p><em>AI Sillicon Roadmap</em></p>
</div>

- **Third-Generation: Chiplets and Grendel:** Tenstorrent is already designing its **third-gen chips using chiplet architecture**. They are developing their own die-to-die interface (similar to UCIe/BoW) to connect chiplets, since they weren’t satisfied with existing standards. This likely means future Tenstorrent products could mix and match **CPU, AI, and IO chiplets**. For inference, this is exciting because they could produce, for example, a chiplet with Tensix cores on a leading-edge node for compute and another with large DRAM or flash for storing giant models, all in one package. Code-names like “Grendel” and “Buda” were mentioned in analysis pieces – Grendel might be a future high-performance core or an IP for licensing. Tenstorrent has also been building out a **RISC-V CPU division** (they acquired a startup to integrate RISC-V cores with their AI fabric). In the context of LLM inference, a Tenstorrent solution in a couple of years could be a multi-chiplet module containing both powerful general-purpose RISC-V cores and Tensix NPU cores. This would facilitate **tight coupling of traditional code and neural nets**, useful for applications that require both (e.g., an AI assistant that also runs logic/rules or post-processing on CPU).

- **Software Unification (Metalium + Compiler):** On the software side, Tenstorrent plans to **merge the bottom-up and top-down stacks** into a cohesive whole. Initially, they had Metalium (low-level) and Buda (high-level) separate, but now they’re converging everything around **MLIR (TT-Forge)**. The ultimate goal is to allow a developer to go from a model definition (PyTorch/TF code) all the way down to highly optimized Metalium kernels **within one compiler flow**. By using MLIR dialects to represent the program at various levels (graph, loop nests, buffer transfers, etc.), they can optimize across traditionally separate layers. For instance, the compiler might automatically generate a custom fused Metalium kernel for a given model’s attention pattern – something that now might require a human expert to code by hand. We already see hints: Tenstorrent’s MLIR-based TT-Forge is their “latest compiler product” and TT-Buda (old stack) is deprecated. The focus now is on **PyTorch-2-TTMetal** smoothness. They have a working `torch.compile` path, but it will improve with time, handling more dynamic model aspects and optimizing memory layouts automatically. **Enhanced automated tiling, scheduling, and multi-device partitioning** are likely to come from this unified compiler.

- **Advanced Inference Features:** The team has explicitly mentioned exploring **speculative decoding** to improve single-batch latency by potentially _\~2×_. Speculative decoding means generating multiple tokens in parallel and then checking which path was correct, thus trading compute for latency. Tenstorrent’s many-core architecture is particularly well-suited to this (as they could dedicate some cores to branch predictions). Implementing this in vLLM-TT or in the model server could significantly cut response time for single-user scenarios, which is critical for competitive chatbot experiences. We expect to see support for such techniques possibly as an update to the inference server (maybe a toggle for enabling speculative or contrastive decoding modes, once they validate it on hardware).

- **New Model Support and Quantization:** On the roadmap will also be supporting **emerging model architectures**. For example, models like MPT, GPT-NeoX, or whatever Meta/GPT-4 open derivatives come out. Given the modularity of their software, adding support typically means writing a few glue functions if the architecture has unique ops. Also, Tenstorrent might integrate **4-bit or sparse inference** if the industry trends that way – their hardware doesn’t natively list 4-bit, but could emulate it or use two 4-bit ops per 8-bit unit to double throughput. If ultra-fast inference at slightly degraded accuracy is desired, they could adopt techniques like quantization-aware finetuning for 4-bit and update TT-Metal to support it. Since they run open-source models, the team is likely tracking things like GPTQ (quantization) and will implement such schemes if customers demand lower precision for even higher throughput.

- **Overcoming Current Blockers:** Present challenges that Tenstorrent is actively addressing include improving the **developer experience and software maturity**. For example, the non-standard memory layout and lack of unified memory means frameworks like _GGML needed custom adaptation_ – Tenstorrent will continue to refine their APIs to hide these differences (perhaps providing seamless tensor views or automatic layout conversion behind the scenes). Reducing data copy overhead (e.g., using DMA engines smartly to overlap host-device transfers with compute) is another software area to optimize. The large number of open issues on TT-metal (2.4k+) indicates many minor features, bugs, or performance asks, which the team will triage – likely focusing on those that unlock key model support or big performance wins. Memory capacity could be a blocker for some extreme cases (e.g. 128K context 70B models might need more than 8 cards to run efficiently). Blackhole’s inclusion of more memory and possibly higher density memory will alleviate that. In the interim, they might implement offloading strategies: for instance, keeping KV caches in host memory for very long contexts and using PCIe streaming. They’ve built a _“HugePages”_ mechanism (mentioned in dstack docs) to optimize host memory usage for such cases. Ensuring stability for long runs (no memory leaks, graceful handling of out-of-memory by spilling to DRAM, etc.) is on the roadmap for a production-grade inference server.

- **Community and Collaboration:** Tenstorrent is leveraging open-source to bring in external contributions (with bounties and an active Discord for developers). Going forward, they’ll likely grow this community – more third-party integrations (maybe HuggingFace pipelines for TT, Kserve, etc.), and academia partnerships to optimize novel models on TT hardware. Their _Open Future_ initiative suggests a philosophy of openness to avoid being blocked by closed systems. So, expect timely support of new open models (OpenLLaMA, etc.), and publishing of standards (they mentioned possibly publishing their chiplet interface standard openly). This open approach can de-risk their adoption: potential buyers see that they are not locked in, and any issues can theoretically be fixed or worked around by the user with access to source.

In summary, Tenstorrent’s future in LLM inference looks promising. With **Blackhole** chips likely boosting per-device performance 2–3× and **software improvements** potentially doubling effective throughput on current hardware, we can project a >5× overall gain in the next generation. That would put an 8-card Tenstorrent system in the range of current 8×H100 in throughput, but at much lower cost – a very disruptive proposition if achieved. Key to watch will be their execution on software: merging Metal and Forge, adding features like speculative decoding, and smoothing all the rough edges for end users. They have identified the right areas (through their transparent communication and engineering interviews), so it’s a matter of engineering time and talent to realize them. Risks remain – e.g., the complexity of their architecture means it’s easy to under-utilize if the compiler isn’t perfect, and competing against Nvidia’s monolithic software ecosystem is non-trivial. But Tenstorrent’s bet is that **open source and a novel architecture can out-innovate** a closed incumbent. If they continue the rapid progress of 2024 into 2025, Tenstorrent could become a preferred platform for organizations looking to deploy LLMs at scale without relying on GPU vendors, with performance that meets or exceeds expectations for real-world AI applications.

## Known Challenges and Conclusion

While Tenstorrent’s LLM inference platform is highly advanced, it’s important to highlight some **known blockers and challenges** that the team is actively working on:

- **Software Maturity & Usability:** The power of Metalium comes at the cost of complexity. As one developer quipped, _“the programming model of Metalium is too different… writing a compiler that can generate good Metalium code would be a PhD thesis”_. Tenstorrent’s own engineers and early adopters have had to invest effort in learning these abstractions. Ensuring that the **TT-Forge compiler automatically handles most optimizations** is essential so that typical users never have to dip into Metalium code. Right now, some model support is marked “preview”, meaning users might hit issues (setup or stability) that require filing an issue and waiting for a fix. The team has been responsive, but it’s a process. The backlog of issues/PRs also reflects this (many are likely feature requests like new ops or enhancements). In short, **developer experience** is a challenge – from installation (setting up drivers, hugepages, etc.) to debugging issues like memory alignment or performance tuning. Tenstorrent is mitigating this by comprehensive docs and tooling (profilers, visualizers), and each software release is ironing out more edge cases. Over 50 contributors on TT-Buda and many on TT-metal indicate a growing talent pool addressing this.

- **Memory Capacity and Bandwidth:** LLM inference needs lots of memory. The first-gen Wormhole cards have 12 GB GDDR6 each, so 8 cards = 96 GB total. This is just enough for a 70B model in BF16 (which would be \~140 GB, hence BF8 is used to halve it). It means **models above 70B or extremely long contexts push limits**. A 128B-parameter model would likely require model parallel >8 or future cards with more memory. The on-chip SRAM (108 MB per chip) is great for working set, but for 128K context, the KV cache for a 70B model can be several GB – which must reside in off-chip GDDR. Transferring that much data can hurt performance (though Tenstorrent’s architecture reduces the pain via parallel transfers). Still, this is a challenge: their current performance data uses context length 128 tokens for fairness. We don’t yet know the throughput hit at 4K or 16K or beyond; presumably it scales down somewhat. This is a risk if customers need giant contexts. The second-gen chips with higher memory bandwidth (LPDDR4 on package, albeit 100 GB/s which is lower than 288 GB/s of GDDR6 on Wormhole but possibly more capacity) might address capacity, and chiplet approach could allow stacking memory. The roadmap to manage memory is something to watch.

- **Competition and Ecosystem:** Nvidia dominates inference with a mature stack and community. Tenstorrent’s open approach is an advantage, but also means they rely on community to some extent for ecosystem (libraries, fine-tuning tools on TT hardware, etc.). If they hit a blocker (e.g. something like a missing Transformer kernel optimization), some customers might not have patience to wait and could fall back to GPUs where tools are abundant. Tenstorrent acknowledges not chasing an cloud API business (thus avoiding direct fight with hyperscalers), but they do need to win over the on-premise and OEM market. This requires not just performance but **reliability and support**. They’ve started offering developer workstations and evaluation kits, which is great. However, any early instability could risk confidence. So far, feedback seems positive (EE Times got a live demo that worked flawlessly, generating answers on the spot). The next year is critical to go from “it works in our lab” to “it works in your data center for months continuously.”

- **Human Resources – Kernel Development:** Writing optimal kernels for a novel architecture is hard. Tenstorrent has a relatively small team compared to Nvidia’s army of engineers optimizing CUDA kernels. They are trying to amplify their effort via community and by focusing on the few patterns that matter most. But if new model architectures introduce new patterns (say a new type of attention or a mixture-of-experts routing), Tenstorrent’s team will need to implement those quickly to keep up. There’s a risk of being spread thin. The open-source strategy is an attempt to crowdsource some of this – indeed, the GGML backend was written by an outside developer, and Tenstorrent’s bounty program on models is tapping external talent. Long-term, if adoption grows, third parties (or even academia) might contribute optimized kernels (similar to how cuBLAS and others have community contributions). But in the short term, it’s largely on Tenstorrent’s own engineers to cover all major ops and models.

- **Integration with Full Workflows:** Many inference deployments involve more than just the raw model. For example, RLHF pipelines, retrieval-augmented generation (RAG), etc. Tenstorrent provides the core model serving, but things like connecting to vector databases, running tokenizers, etc., still happen on CPU. With RISC-V CPUs coming into their chips, they plan to bring more of that in-house. For now, a risk is that if those parts become bottlenecks (for instance, a slow tokenizer on CPU might limit an otherwise fast model throughput), the overall system might underperform. They’ll need to ensure that the CPU side (host side) is not a weak link – possibly by optimizing the Python server code or by moving some preprocessing onto the device (they did talk about small RISC-V “baby cores” on chip which could potentially run light tasks).

**Conclusion:** Tenstorrent’s efforts in LLM inference represent a bold and technically sophisticated approach to AI acceleration. They have architected a system that is _different by design_ – many independent cores with explicit software control, instead of monolithic GPU streaming multiprocessors. This has necessitated a rich software stack from scratch, which they have built and open-sourced in a relatively short time. The result so far is that **Tenstorrent can run cutting-edge large language models with competitive performance and excellent scalability**, using commodity hardware that they sell openly. The technical deep dive reveals the clever strategies (like BF8 precision, non-standard memory layouts, parallel kernel execution) that make this possible, as well as the areas where continued work is ongoing (compiler automation, further performance tuning).

For engineers and technical leads evaluating Tenstorrent, the key takeaways are: you’ll get an _unprecedented level of visibility and control_ over the inference process (down to writing kernels if desired), you can deploy popular models like LLaMA, Qwen, etc. on the hardware today with near state-of-the-art speed, but you should be prepared for a learning curve and an active partnership with Tenstorrent’s fast-evolving software. The company’s roadmap is encouraging – with hardware improvements and unified software on the horizon, the gap between Tenstorrent and more established solutions is closing rapidly. In some niches (like multi-model ensembles, or strictly on-prem open-source requirements), Tenstorrent may already be the superior choice. Risks around software maturity are being addressed with each release, and the open-source model means any critical issue can theoretically be fixed by the user or community if needed (no black boxes).

In conclusion, Tenstorrent’s LLM inference platform is **one to watch (and experiment with) for any organization aiming to deploy large AI models at scale**. It offers a compelling blend of performance, openness, and cost-efficiency. The technical depth we explored – from Tensix core design to TT-Metal code – shows that this is a serious engineering effort, not just repackaging of standard IP. If Tenstorrent continues to execute well, they could significantly reshape the LLM infrastructure landscape, providing a viable alternative to GPU-centric inference with unique advantages for those willing to embrace a new paradigm.

## References

1. **EE Times: Tenstorrent Engineers Talk Open-Sourced Bare-Metal Stack**
   [https://www.eetimes.com/tenstorrent-engineers-talk-open-sourced-bare-metal-stack/](https://www.eetimes.com/tenstorrent-engineers-talk-open-sourced-bare-metal-stack/)
   Overview of Tenstorrent’s open-source strategy, Metalium programming model, Tensix core architecture, and Galaxy multi-chip systems.

2. **EE Times: Tenstorrent Shows Off Single-User LLM Speed For Workstation**
   [https://www.eetimes.com/tenstorrent-shows-off-single-user-llm-speed-for-workstation/](https://www.eetimes.com/tenstorrent-shows-off-single-user-llm-speed-for-workstation/)
   Details on Loud Box demo with LLaMA-70B, single-user token speed benchmarks, and Tenstorrent’s competitive positioning against Nvidia H100.

3. **Tenstorrent/tt-metal GitHub Repository**
   [https://github.com/tenstorrent/tt-metal](https://github.com/tenstorrent/tt-metal)
   Main software stack for neural network runtime, operators, kernels, and performance benchmarks on Tenstorrent hardware.

4. **Tenstorrent/tt-inference-server GitHub Repository**
   [https://github.com/tenstorrent/tt-inference-server](https://github.com/tenstorrent/tt-inference-server)
   Repository providing LLM model serving APIs, vLLM integration, supported models table, and deployment examples.

5. **Tenstorrent Wormhole Product Page**
   [https://tenstorrent.com/products/wormhole/](https://tenstorrent.com/products/wormhole/)
   Specifications for Wormhole PCIe AI cards including Tensix core counts, memory, networking, and inference use cases.

6. **Tenstorrent/tt-forge GitHub Repository**
   [https://github.com/tenstorrent/tt-forge](https://github.com/tenstorrent/tt-forge)
   MLIR-based compiler stack for lowering models from PyTorch, ONNX, or TensorFlow to Tenstorrent executable kernels.

7. **Meta LLaMA-3.3-70B Model Card (Hugging Face)**
   [https://huggingface.co/meta-llama/Llama-3-70b](https://huggingface.co/meta-llama/Llama-3-70b)
   Official description of model architecture, parameters, and intended use cases.

8. **Alibaba Qwen2.5-72B Model Card (Hugging Face)**
   [https://huggingface.co/Qwen/Qwen2.5-72B](https://huggingface.co/Qwen/Qwen2.5-72B)
   Overview of Qwen bilingual LLM with 128K context support, training data sources, and performance claims.

9. **DeepSeek R1 Model Card**
   [https://huggingface.co/deepseek-ai/DeepSeek-V2](https://huggingface.co/deepseek-ai/DeepSeek-V2)
   Technical description of DeepSeek R1 and distilled variants, focusing on reasoning abilities and scaling benchmarks.

10. **Tenstorrent Developer Documentation – TT-Metal and Metalium Programming Guides**
    [https://tenstorrent-metal.readthedocs.io/](https://tenstorrent-metal.readthedocs.io/)
    Detailed API references, kernel programming model, data layout specifications, and hardware interfacing instructions.

11. **Tenstorrent/tt-tvm GitHub Repository**
    [https://github.com/tenstorrent/tt-tvm](https://github.com/tenstorrent/tt-tvm)
    Integration for TVM compiler stack, enabling model deployment on Tenstorrent via TVM lowering.

12. **Tenstorrent/tt-xla GitHub Repository**
    [https://github.com/tenstorrent/tt-xla](https://github.com/tenstorrent/tt-xla)
    PJRT device backend for XLA/JAX models targeting Tenstorrent accelerators.

13. **Tenstorrent/tt-buda GitHub Repository (deprecated)**
    [https://github.com/tenstorrent/tt-buda](https://github.com/tenstorrent/tt-buda)
    Earlier compiler stack now replaced by TT-Forge, included for historical context.

14. **Tenstorrent/tt-topology GitHub Repository**
    [https://github.com/tenstorrent/tt-topology](https://github.com/tenstorrent/tt-topology)
    Tools for configuring multi-card Ethernet topology in Galaxy and multi-node setups.

15. **Tenstorrent/tt-flash GitHub Repository**
    [https://github.com/tenstorrent/tt-flash](https://github.com/tenstorrent/tt-flash)
    Firmware flashing tools for Tenstorrent PCIe cards.

16. **Tenstorrent/tt-smi GitHub Repository**
    [https://github.com/tenstorrent/tt-smi](https://github.com/tenstorrent/tt-smi)
    Command-line monitoring tool for card health, utilization, and debugging (similar to NVIDIA’s nvidia-smi).

17. **Tenstorrent Performance Tables (TT-Metal README)**
    [https://github.com/tenstorrent/tt-metal#performance](https://github.com/tenstorrent/tt-metal#performance)
    Token-per-second benchmarks, multi-device scaling results, and precision configuration data for LLM inference.

18. **Tenstorrent YouTube Developer Day (2024)**
    [https://www.youtube.com/@Tenstorrent](https://www.youtube.com/@Tenstorrent)
    Recorded demos and talks explaining Metalium programming, compiler internals, and real-time model inference showcases.
